from py_captions_for_channels.logging.structured_logger import get_logger
import re
import subprocess
from typing import Callable, Optional
import time
from pathlib import Path
import sys
import threading

from .config import (
    LOG_DIVIDER_CHAR,
    LOG_DIVIDER_LENGTH,
    LOG_STATS_ENABLED,
    PIPELINE_TIMEOUT,
)
from .progress_tracker import get_progress_tracker
from .execution_tracker import get_tracker


class PipelineResult:
    """Result of a pipeline execution."""

    def __init__(
        self,
        success: bool,
        returncode: int,
        stdout: str = "",
        stderr: str = "",
        command: str = "",
        elapsed_seconds: float = 0.0,
        output_files: dict = None,
        input_size_bytes: int = 0,
        input_path: str = "",
    ):
        self.success = success
        self.returncode = returncode
        self.stdout = stdout
        self.stderr = stderr
        self.command = command
        self.elapsed_seconds = elapsed_seconds
        self.output_files = output_files or {}  # Dict of {filename: size_bytes}
        self.input_size_bytes = input_size_bytes
        self.input_path = input_path
        self.is_dry_run = False  # Set to True for dry-run executions

    def get_total_output_size(self) -> int:
        """Return total size of all output files in bytes."""
        return sum(self.output_files.values())

    def format_size(self, bytes_size: int) -> str:
        """Format bytes to human-readable string."""
        for unit in ["B", "KB", "MB", "GB"]:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f} {unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f} TB"


class Pipeline:
    """Executes the captioning workflow.

    Supports dry-run mode for testing and captures stdout/stderr
    for debugging and logging.
    """

    def __init__(self, command_template: str, dry_run: bool = False):
        """Initialize pipeline.

        Args:
            command_template: Command template with {path} placeholder
            dry_run: If True, print commands instead of executing them
        """
        self.command_template = command_template
        self.dry_run = dry_run

    def _collect_output_files(self, recording_path: str) -> dict:
        """Collect statistics for output files generated by the command.

        Looks for:
        - .srt caption file (same directory/name as recording)
        - .mp4 transcoded file (if TRANSCODE_FOR_FIRETV=true)

        Args:
            recording_path: Path to the original recording file

        Returns:
            Dict of {filename: size_bytes} for generated files
        """
        output_files = {}
        recording_path = Path(recording_path)
        recording_dir = recording_path.parent
        base_name = recording_path.stem  # filename without extension

        # Check for .srt caption file
        srt_file = recording_dir / f"{base_name}.srt"
        if srt_file.exists():
            output_files[srt_file.name] = srt_file.stat().st_size

        # Check for .mp4 transcoded file
        mp4_file = recording_dir / f"{base_name}.mp4"
        if mp4_file.exists():
            output_files[mp4_file.name] = mp4_file.stat().st_size

        return output_files

    def _parse_whisper_progress(self, line: str) -> Optional[float]:
        """Parse progress percentage from Whisper output.

        Whisper outputs lines like:
        [00:01.000 --> 00:05.000]  transcribed text...
        We can estimate progress based on timestamp vs duration.

        Returns:
            Progress percentage (0-100) or None if not parseable
        """
        # Look for timestamp patterns like [00:01.000 --> 00:05.000]
        match = re.search(
            r"\[(\d{2}):(\d{2})\.(\d{3})\s*-->\s*(\d{2}):(\d{2})\.(\d{3})\]",
            line,
        )
        if match:
            # Parse end timestamp (hours:minutes.milliseconds)
            end_min = int(match.group(4))
            end_sec = int(match.group(5))
            end_ms = int(match.group(6))
            end_time_seconds = end_min * 60 + end_sec + end_ms / 1000.0
            return end_time_seconds  # Return time processed, caller will convert to %
        return None

    def _parse_ffmpeg_progress(self, line: str) -> Optional[dict]:
        """Parse progress from FFmpeg output.

        FFmpeg outputs lines like:
        frame= 1234 fps= 45 q=28.0 size=   12345kB time=00:01:23.45
        bitrate=1234.5kbits/s speed=1.23x

        Returns:
            Dict with time, speed, etc. or None if not parseable
        """
        # Look for time= pattern
        time_match = re.search(r"time=(\d{2}):(\d{2}):(\d{2})\.(\d{2})", line)
        speed_match = re.search(r"speed=\s*([0-9.]+)x", line)

        if time_match:
            hours = int(time_match.group(1))
            minutes = int(time_match.group(2))
            seconds = int(time_match.group(3))
            time_seconds = hours * 3600 + minutes * 60 + seconds

            result = {"time_seconds": time_seconds}
            if speed_match:
                result["speed"] = float(speed_match.group(1))
            return result
        return None

    def _stream_and_capture_output(
        self,
        proc: subprocess.Popen,
        job_id: str,
        log,
        input_duration: float = 0.0,
        cancel_check: Optional[Callable[[], bool]] = None,
    ) -> tuple:
        """Stream stdout/stderr in real-time, parse progress, and capture output.

        Args:
            proc: Running subprocess.Popen
            job_id: Job identifier for progress tracking
            log: Logger instance
            input_duration: Input file duration in seconds (for progress calculation)
            cancel_check: Optional callable to check if job should be cancelled

        Returns:
            Tuple of (stdout, stderr) as strings
        """
        progress_tracker = get_progress_tracker()
        stdout_lines = []
        stderr_lines = []

        # Get job number for progress display
        job_number = None
        try:
            tracker = get_tracker()
            execution = tracker.get_execution(job_id)
            if execution and execution.get("job_number"):
                job_number = execution.get("job_number")
        except Exception:
            pass  # If we can't get job number, just continue without it

        current_process_type = (
            "whisper"  # Start with whisper, switch to ffmpeg when detected
        )
        last_whisper_time = 0.0

        def read_stream(stream, is_stderr: bool):
            nonlocal current_process_type, last_whisper_time
            for line in iter(stream.readline, ""):
                if not line:
                    break

                line = line.strip()
                if is_stderr:
                    stderr_lines.append(line)
                else:
                    stdout_lines.append(line)

                # Parse progress from line
                if is_stderr:  # FFmpeg writes to stderr
                    ffmpeg_progress = self._parse_ffmpeg_progress(line)
                    if ffmpeg_progress:
                        current_process_type = "ffmpeg"
                        if input_duration > 0:
                            percent = min(
                                100.0,
                                (ffmpeg_progress["time_seconds"] / input_duration)
                                * 100,
                            )
                            speed = ffmpeg_progress.get("speed", 0)
                            job_prefix = f"#{job_number} " if job_number else ""
                            progress_tracker.update_progress(
                                job_id,
                                "ffmpeg",
                                percent,
                                (
                                    f"{job_prefix}Encoding... {speed:.1f}x"
                                    if speed > 0
                                    else f"{job_prefix}Encoding..."
                                ),
                                ffmpeg_progress,
                            )
                        else:
                            # Don't know duration, just show time processed
                            job_prefix = f"#{job_number} " if job_number else ""
                            time_sec = ffmpeg_progress["time_seconds"]
                            progress_tracker.update_progress(
                                job_id,
                                "ffmpeg",
                                0,
                                f"{job_prefix}{time_sec:.1f}s processed",
                                ffmpeg_progress,
                            )

                # Whisper outputs to stdout
                if not is_stderr:
                    whisper_time = self._parse_whisper_progress(line)
                    if whisper_time is not None:
                        current_process_type = "whisper"
                        last_whisper_time = whisper_time
                        job_prefix = f"#{job_number} " if job_number else ""
                        if input_duration > 0:
                            percent = min(100.0, (whisper_time / input_duration) * 100)
                            progress_tracker.update_progress(
                                job_id,
                                "whisper",
                                percent,
                                f"{job_prefix}Transcribing... {whisper_time:.1f}s",
                                {"time_seconds": whisper_time},
                            )
                        else:
                            progress_tracker.update_progress(
                                job_id,
                                "whisper",
                                0,
                                f"{job_prefix}{whisper_time:.1f}s transcribed",
                                {"time_seconds": whisper_time},
                            )

        # Read stdout and stderr in separate threads
        stdout_thread = threading.Thread(target=read_stream, args=(proc.stdout, False))
        stderr_thread = threading.Thread(target=read_stream, args=(proc.stderr, True))

        stdout_thread.daemon = True
        stderr_thread.daemon = True

        stdout_thread.start()
        stderr_thread.start()

        # Wait for process to complete or timeout
        start_time = time.time()
        while proc.poll() is None:
            # Check for cancellation
            if cancel_check and cancel_check():
                log.warning("Job cancellation requested, terminating process")
                proc.terminate()
                try:
                    proc.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    log.error("Process did not terminate gracefully, killing")
                    proc.kill()
                    proc.wait()
                break

            # Check for timeout
            elapsed = time.time() - start_time
            if elapsed > PIPELINE_TIMEOUT:
                log.error(
                    "Process timeout after %d seconds, terminating", PIPELINE_TIMEOUT
                )
                proc.terminate()
                try:
                    proc.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    log.error("Process did not terminate gracefully, killing")
                    proc.kill()
                    proc.wait()
                break

            time.sleep(0.1)

        # Wait for reader threads to finish
        stdout_thread.join(timeout=2)
        stderr_thread.join(timeout=2)

        return "\n".join(stdout_lines), "\n".join(stderr_lines)

    def _probe_input_duration_seconds(self, job_id_or_path: str) -> float:
        """Probe input media duration using ffprobe.

        Accepts either the recording path (preferred) or job_id which contains
        the title and timestamp; in our case we will attempt to parse the path
        from the current job ID when only job_id is available, but primarily
        this should be called with the recording path.

        Returns duration in seconds, or 0 if not available.
        """
        # If a path string was passed, use it directly
        path = None
        p = Path(job_id_or_path)
        if p.exists():
            path = str(p)
        else:
            # Not a valid path; cannot probe
            return 0.0

        try:
            # Run ffprobe to get duration
            proc = subprocess.run(
                [
                    "ffprobe",
                    "-v",
                    "error",
                    "-show_entries",
                    "format=duration",
                    "-of",
                    "default=noprint_wrappers=1:nokey=1",
                    path,
                ],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if proc.returncode != 0:
                return 0.0
            out = proc.stdout.strip()
            try:
                return float(out)
            except Exception:
                return 0.0
        except Exception:
            return 0.0

    def _log_job_statistics(self, result: PipelineResult, job_id: str, log):
        """Log job completion statistics with visual divider.

        Args:
            result: PipelineResult with execution details
            job_id: Job identifier for context
        """
        if not result.success:
            return  # Only log stats for successful jobs

        # Format timing
        minutes, seconds = divmod(result.elapsed_seconds, 60)
        time_str = f"{int(minutes):02d}:{seconds:05.2f}"

        # Probe input duration for real-time factor
        # Input duration from original media
        input_duration = 0.0
        if getattr(result, "input_path", None):
            input_duration = self._probe_input_duration_seconds(result.input_path)
        if input_duration and input_duration > 0:
            rt_factor = input_duration / max(result.elapsed_seconds, 0.001)
        else:
            rt_factor = None

        # Format file sizes
        total_size = result.get_total_output_size()
        total_size_str = result.format_size(total_size)

        # Build stats lines
        divider = LOG_DIVIDER_CHAR * LOG_DIVIDER_LENGTH
        stats_lines = [
            divider,  # Visual divider
            f"Job Statistics: {job_id}",
            f"  Processing Time: {time_str}",
            f"  Total Output Size: {total_size_str}",
        ]

        # Input file size and duration
        if hasattr(result, "input_size_bytes"):
            stats_lines.append(
                f"  Input File Size: {result.format_size(result.input_size_bytes)}"
            )
        if input_duration and input_duration > 0:
            m, s = divmod(input_duration, 60)
            stats_lines.append(f"  Input Duration: {int(m):02d}:{s:05.2f}")
        if rt_factor:
            stats_lines.append(f"  Real-Time Factor: {rt_factor:.1f}x")

        # Input file size
        if hasattr(result, "input_size_bytes"):
            stats_lines.append(
                f"  Input File Size: {result.format_size(result.input_size_bytes)}"
            )

        # Add per-file statistics if multiple files
        if result.output_files:
            stats_lines.append("  Output Files:")
            for filename, size_bytes in sorted(result.output_files.items()):
                size_str = result.format_size(size_bytes)
                stats_lines.append(f"    - {filename}: {size_str}")

        stats_lines.append(divider)

        # Log as single info message (job ID will be added by formatter)
        if LOG_STATS_ENABLED:
            log.info("\n" + "\n".join(stats_lines))

    def run(
        self,
        event,
        job_id_override: Optional[str] = None,
        cancel_check: Optional[Callable[[], bool]] = None,
    ) -> PipelineResult:
        """Execute the caption command for the given event.

        Args:
            event: ProcessingEvent with path, title, etc.

        Returns:
            PipelineResult with execution details
        """
        # Set job ID for log formatting
        timestamp = getattr(event, "timestamp", None)
        if isinstance(timestamp, str):
            timestamp_str = timestamp
        elif timestamp:
            timestamp_str = timestamp.strftime("%H:%M:%S")
        else:
            timestamp_str = "UNKNOWN"
        job_id = job_id_override or f"{event.title} @ {timestamp_str}"
        log = get_logger("pipeline", job_id=job_id)

        # Clear any stale progress from previous runs
        progress_tracker = get_progress_tracker()
        progress_tracker.clear_progress(job_id)

        start_time = time.time()
        try:
            # Format command with safe-quoted event path to handle spaces/special chars
            try:
                import shlex

                safe_path = shlex.quote(event.path)
            except Exception:
                safe_path = event.path  # fallback

            # Check if we're using the embed_captions module (production case)
            # If so, build command dynamically with settings from event
            # Otherwise, use the command_template (for tests and custom commands)
            # Match both: "py_captions_for_channels.embed_captions" (module)
            # and "/path/to/embed_captions.py" (direct script)
            if (
                "py_captions_for_channels.embed_captions" in self.command_template
                or "embed_captions.py" in self.command_template
            ):
                # Build command with settings from event or config
                whisper_model = getattr(event, "whisper_model", "medium")
                log_verbosity = getattr(event, "log_verbosity", "NORMAL")
                skip_caption_generation = getattr(
                    event, "skip_caption_generation", False
                )
                srt_path = getattr(event, "srt_path", None)
                if not srt_path:
                    # Default SRT path: same dir as input, basename.srt
                    import os

                    base = os.path.splitext(os.path.basename(event.path))[0]
                    srt_path = os.path.join(os.path.dirname(event.path), f"{base}.srt")

                # Shell-quote arguments safely
                def shell_quote(val):
                    return shlex.quote(str(val)) if val is not None else "''"

                # Python executable doesn't need quoting for subprocess.run(shell=True)
                python_exec = sys.executable
                options = [
                    f"--input {shell_quote(event.path)}",
                    f"--srt {shell_quote(srt_path)}",
                    f"--job-id {shell_quote(job_id)}",
                    f"--model {shell_quote(whisper_model)}" if whisper_model else "",
                    (
                        f"--verbosity {shell_quote(log_verbosity)}"
                        if log_verbosity
                        else ""
                    ),
                    "--skip-caption-generation" if skip_caption_generation else "",
                ]
                options_str = " ".join([opt for opt in options if opt])
                cmd = (
                    f"{python_exec} -m py_captions_for_channels.embed_captions "
                    f"{options_str}"
                )
            else:
                # Use traditional command template formatting
                # (for tests/custom commands)
                cmd = self.command_template.format(path=safe_path)

            if self.dry_run:
                log.info("[DRY-RUN] Would execute: %s", cmd)
                log.info("[DRY-RUN] Event: %s (path=%s)", event.title, event.path)
                result = PipelineResult(
                    success=True,
                    returncode=0,
                    stdout="",
                    stderr="",
                    command=cmd,
                    elapsed_seconds=0.0,
                    input_path=event.path,
                )
                # Mark result as dry-run so tracker can handle it specially
                result.is_dry_run = True
                return result

            # Visual divider at job start (configurable)
            if LOG_STATS_ENABLED:
                log.info("\n" + (LOG_DIVIDER_CHAR * LOG_DIVIDER_LENGTH))

            # Get job number for markers
            job_number = None
            try:
                tracker = get_tracker()
                execution = tracker.get_execution(job_id)
                if execution and execution.get("job_number"):
                    job_number = execution.get("job_number")
            except Exception:
                pass  # Continue without job number if unavailable

            job_marker = f"Job #{job_number}" if job_number else f"Job {job_id[:8]}"
            log.info("=" * 80)
            log.info(f"START {job_marker}: {event.title}")
            log.info("=" * 80)
            log.info("Running caption pipeline: %s", cmd)

            # Get input duration for progress tracking
            input_duration = self._probe_input_duration_seconds(event.path)

            try:
                # Execute command with subprocess.Popen() to stream output in real-time
                log.debug("About to execute command: %s", cmd)

                try:
                    proc = subprocess.Popen(
                        cmd,
                        shell=True,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True,
                        bufsize=1,  # Line buffered
                    )

                    # Stream output and parse progress
                    stdout, stderr = self._stream_and_capture_output(
                        proc, job_id, log, input_duration, cancel_check
                    )

                    returncode = proc.returncode

                    # Clear progress after completion
                    progress_tracker = get_progress_tracker()
                    progress_tracker.clear_progress(job_id)

                except subprocess.TimeoutExpired as e:
                    elapsed = time.time() - start_time
                    log.error(
                        "Caption pipeline timed out for %s after %d seconds",
                        event.path,
                        PIPELINE_TIMEOUT,
                    )
                    # Clear progress
                    progress_tracker = get_progress_tracker()
                    progress_tracker.clear_progress(job_id)
                    log.info("=" * 80)
                    log.info(f"END {job_marker}: TIMEOUT after {elapsed:.1f}s")
                    log.info("=" * 80)
                    return PipelineResult(
                        success=False,
                        returncode=-1,
                        stdout=e.stdout or "",
                        stderr=f"Command timed out after {PIPELINE_TIMEOUT} seconds",
                        command=cmd,
                        elapsed_seconds=elapsed,
                        input_path=event.path,
                    )

                if returncode != 0:
                    elapsed = time.time() - start_time
                    log.error(
                        "Caption pipeline failed for %s (exit code %d)",
                        event.path,
                        returncode,
                    )
                    log.error("Command attempted: %s", cmd)
                    if returncode == 126:
                        log.error(
                            "Exit code 126: Permission denied or not executable. "
                            "Check permissions and shebang for: %s",
                            cmd,
                        )
                    if stderr:
                        log.error("stderr: %s", stderr[:500])
                    log.info("=" * 80)
                    end_msg = (
                        f"END {job_marker}: FAILED "
                        f"(exit {returncode}) after {elapsed:.1f}s"
                    )
                    log.info(end_msg)
                    log.info("=" * 80)
                    return PipelineResult(
                        success=False,
                        returncode=returncode,
                        stdout=stdout,
                        stderr=stderr,
                        command=cmd,
                        elapsed_seconds=elapsed,
                        input_path=event.path,
                    )
                else:
                    log.info("Caption pipeline completed for %s", event.path)
                    # Log beginning and end of output to capture optimization
                    if stdout:
                        log.debug("stdout: %s", stdout[-1000:])
                    if stderr:
                        # Show beginning, optimization, and end
                        if len(stderr) > 5000:
                            log.info("whisper output (beginning): %s", stderr[:2000])
                            # Extract optimization logs specifically
                            opt_lines = [
                                line
                                for line in stderr.split("\n")
                                if "[OPTIMIZATION]" in line
                            ]
                            if opt_lines:
                                log.info(
                                    "optimization params: %s", "\n".join(opt_lines)
                                )
                            log.info("whisper output (end): %s", stderr[-1000:])
                        else:
                            log.info("whisper output: %s", stderr)

                    # Collect output file statistics
                    elapsed = time.time() - start_time
                    output_files = self._collect_output_files(event.path)
                    # Parse ffmpeg speed if present
                    ffmpeg_speed = None
                    m = re.search(r"speed=([0-9.]+)x", stderr)
                    if m:
                        ffmpeg_speed = float(m.group(1))
                    try:
                        input_size = Path(event.path).stat().st_size
                    except Exception:
                        input_size = 0

                    log.info("=" * 80)
                    log.info(f"END {job_marker}: SUCCESS in {elapsed:.1f}s")
                    log.info("=" * 80)

                    return PipelineResult(
                        success=True,
                        returncode=0,
                        stdout=stdout,
                        stderr=stderr,
                        command=cmd,
                        elapsed_seconds=elapsed,
                        output_files=output_files,
                        input_size_bytes=input_size,
                        input_path=event.path,
                    )

                    # If ffmpeg speed parsed, log it for reference
                    if ffmpeg_speed is not None:
                        log.info(
                            "Transcode speed reported by ffmpeg: %.2fx", ffmpeg_speed
                        )

            except Exception as e:
                elapsed = time.time() - start_time
                log.error("Exception running caption pipeline: %s", e)
                log.error("Command attempted: %s", cmd)

                # Clean up subprocess if still running to prevent orphans
                try:
                    if proc and proc.poll() is None:
                        log.warning("Terminating subprocess due to exception")
                        proc.terminate()
                        try:
                            proc.wait(timeout=5)
                        except subprocess.TimeoutExpired:
                            log.warning("Subprocess didn't terminate, killing it")
                            proc.kill()
                            proc.wait()
                except Exception as cleanup_err:
                    log.error("Error cleaning up subprocess: %s", cleanup_err)

                # Clear progress tracking
                try:
                    progress_tracker.clear_progress(job_id)
                except Exception as progress_err:
                    log.error(
                        "Error clearing progress after exception: %s", progress_err
                    )

                log.info("=" * 80)
                log.info(f"END {job_marker}: EXCEPTION after {elapsed:.1f}s")
                log.info("=" * 80)

                return PipelineResult(
                    success=False,
                    returncode=-1,
                    stdout="",
                    stderr=str(e),
                    command=cmd,
                    elapsed_seconds=elapsed,
                    input_path=event.path,
                )
        finally:
            pass  # No job_id context to clear with new logger
