import subprocess
import logging
from .logging_config import set_job_id

LOG = logging.getLogger(__name__)


import subprocess
import logging
import time
from pathlib import Path
from .logging_config import set_job_id


class PipelineResult:
    """Result of a pipeline execution."""

    def __init__(
        self,
        success: bool,
        returncode: int,
        stdout: str = "",
        stderr: str = "",
        command: str = "",
        elapsed_seconds: float = 0.0,
        output_files: dict = None,
        input_size_bytes: int = 0,
    ):
        self.success = success
        self.returncode = returncode
        self.stdout = stdout
        self.stderr = stderr
        self.command = command
        self.elapsed_seconds = elapsed_seconds
        self.output_files = output_files or {}  # Dict of {filename: size_bytes}
        self.input_size_bytes = input_size_bytes

    def get_total_output_size(self) -> int:
        """Return total size of all output files in bytes."""
        return sum(self.output_files.values())

    def format_size(self, bytes_size: int) -> str:
        """Format bytes to human-readable string."""
        for unit in ["B", "KB", "MB", "GB"]:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f} {unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f} TB"

class Pipeline:
    """Executes the captioning workflow.

    Supports dry-run mode for testing and captures stdout/stderr
    for debugging and logging.
    """

    def __init__(self, command_template: str, dry_run: bool = False):
        """Initialize pipeline.

        Args:
            command_template: Command template with {path} placeholder
            dry_run: If True, print commands instead of executing them
        """
        self.command_template = command_template
        self.dry_run = dry_run

    def _collect_output_files(self, recording_path: str) -> dict:
        """Collect statistics for output files generated by the command.

        Looks for:
        - .srt caption file (same directory/name as recording)
        - .mp4 transcoded file (if TRANSCODE_FOR_FIRETV=true)

        Args:
            recording_path: Path to the original recording file

        Returns:
            Dict of {filename: size_bytes} for generated files
        """
        output_files = {}
        recording_path = Path(recording_path)
        recording_dir = recording_path.parent
        base_name = recording_path.stem  # filename without extension

        # Check for .srt caption file
        srt_file = recording_dir / f"{base_name}.srt"
        if srt_file.exists():
            output_files[srt_file.name] = srt_file.stat().st_size

        # Check for .mp4 transcoded file
        mp4_file = recording_dir / f"{base_name}.mp4"
        if mp4_file.exists():
            output_files[mp4_file.name] = mp4_file.stat().st_size

        return output_files

    def _log_job_statistics(self, result: PipelineResult, job_id: str) -> None:
        """Log job completion statistics with visual divider.

        Args:
            result: PipelineResult with execution details
            job_id: Job identifier for context
        """
        if not result.success:
            return  # Only log stats for successful jobs

        # Format timing
        minutes, seconds = divmod(result.elapsed_seconds, 60)
        time_str = f"{int(minutes):02d}:{seconds:05.2f}"

        # Format file sizes
        total_size = result.get_total_output_size()
        total_size_str = result.format_size(total_size)

        # Build stats lines
        stats_lines = [
            "--" * 20,  # Visual divider
            f"Job Statistics: {job_id}",
            f"  Processing Time: {time_str}",
            f"  Total Output Size: {total_size_str}",
        ]

        # Input file size
        if hasattr(result, "input_size_bytes"):
            stats_lines.append(
                f"  Input File Size: {result.format_size(result.input_size_bytes)}"
            )

        # Add per-file statistics if multiple files
        if result.output_files:
            stats_lines.append("  Output Files:")
            for filename, size_bytes in sorted(result.output_files.items()):
                size_str = result.format_size(size_bytes)
                stats_lines.append(f"    - {filename}: {size_str}")

        stats_lines.append("--" * 20)

        # Log as single info message (job ID will be added by formatter)
        LOG.info("\n" + "\n".join(stats_lines))

    def __init__(self, command_template: str, dry_run: bool = False):
        """Initialize pipeline.

        Args:
            command_template: Command template with {path} placeholder
            dry_run: If True, print commands instead of executing them
        """
        self.command_template = command_template
        self.dry_run = dry_run

    def run(self, event) -> PipelineResult:
        """Execute the caption command for the given event.

        Args:
            event: ProcessingEvent with path, title, etc.

        Returns:
            PipelineResult with execution details
        """
        # Set job ID for log formatting
        timestamp = getattr(event, "timestamp", None)
        timestamp_str = timestamp.strftime("%H:%M:%S") if timestamp else "UNKNOWN"
        job_id = f"{event.title} @ {timestamp_str}"
        set_job_id(job_id)

        start_time = time.time()
        try:
            # Format command with safe-quoted event path to handle spaces/special chars
            try:
                import shlex
                safe_path = shlex.quote(event.path)
            except Exception:
                safe_path = event.path  # fallback
            cmd = self.command_template.format(path=safe_path)

            if self.dry_run:
                LOG.info("[DRY-RUN] Would execute: %s", cmd)
                LOG.info("[DRY-RUN] Event: %s (path=%s)", event.title, event.path)
                return PipelineResult(
                    success=True,
                    returncode=0,
                    stdout="",
                    stderr="",
                    command=cmd,
                )

            # Visual divider at job start
            LOG.info("\n" + "-" * 40)
            LOG.info("Running caption pipeline: %s", cmd)

            try:
                # Execute command and capture output
                proc = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=3600,  # 1 hour timeout
                )

                if proc.returncode != 0:
                    elapsed = time.time() - start_time
                    LOG.error(
                        "Caption pipeline failed for %s (exit code %d)",
                        event.path,
                        proc.returncode,
                    )
                    if proc.stderr:
                        LOG.error("stderr: %s", proc.stderr[:500])
                    return PipelineResult(
                        success=False,
                        returncode=proc.returncode,
                        stdout=proc.stdout,
                        stderr=proc.stderr,
                        command=cmd,
                        elapsed_seconds=elapsed,
                    )
                else:
                    LOG.info("Caption pipeline completed for %s", event.path)
                    # Log whisper's output for debugging
                    if proc.stdout:
                        LOG.debug("stdout: %s", proc.stdout[-1000:])
                    if proc.stderr:
                        LOG.info("whisper output: %s", proc.stderr[-500:])

                    # Collect output file statistics
                    elapsed = time.time() - start_time
                    output_files = self._collect_output_files(event.path)
                    try:
                        input_size = Path(event.path).stat().st_size
                    except Exception:
                        input_size = 0

                    return PipelineResult(
                        success=True,
                        returncode=0,
                        stdout=proc.stdout,
                        stderr=proc.stderr,
                        command=cmd,
                        elapsed_seconds=elapsed,
                        output_files=output_files,
                        input_size_bytes=input_size,
                    )

            except subprocess.TimeoutExpired:
                elapsed = time.time() - start_time
                LOG.error("Caption pipeline timed out for %s", event.path)
                return PipelineResult(
                    success=False,
                    returncode=-1,
                    stdout="",
                    stderr="Command timed out after 3600 seconds",
                    command=cmd,
                    elapsed_seconds=elapsed,
                )
            except Exception as e:
                elapsed = time.time() - start_time
                LOG.error("Exception running caption pipeline: %s", e)
                return PipelineResult(
                    success=False,
                    returncode=-1,
                    stdout="",
                    stderr=str(e),
                    command=cmd,
                    elapsed_seconds=elapsed,
                )
        finally:
            # Clear job ID after processing
            set_job_id(None)
